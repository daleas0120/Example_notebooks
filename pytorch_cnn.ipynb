{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq9DZen1CMaSydv2HSiDED",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daleas0120/Example_notebooks/blob/main/pytorch_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "How To Invert a Neural Network\n",
        "\n",
        "---\n",
        "\n",
        "Curious to see if I can take a neural network, train it, and then run it backwards.\n"
      ],
      "metadata": {
        "id": "0CbQ5w2aBo33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyBr4GQUB2RK",
        "outputId": "be46fc39-6f3d-4909-d8ca-46eaba36466e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfv0LSB-pQbK",
        "outputId": "5941af41-0024-4341-f14f-e430b9377e7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9-NoWsGDBmSq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torcheval.metrics.functional import multiclass_f1_score\n",
        "\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size_train = 8\n",
        "batch_size_test = 8"
      ],
      "metadata": {
        "id": "dmkgv4S1BvY6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Load Data\n"
      ],
      "metadata": {
        "id": "nePIZII6Byzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=False)"
      ],
      "metadata": {
        "id": "mSXVIcVOBwq1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Custom, Random Dataset"
      ],
      "metadata": {
        "id": "_AdZX9xDSwKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_img = torch.rand(1, 28, 28, requires_grad=True)"
      ],
      "metadata": {
        "id": "lwfdKaEhXJfP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define Model"
      ],
      "metadata": {
        "id": "Oug41hP4CA82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(CNN_classifier, self).__init__()\n",
        "        self.conv0 = nn.Conv2d(1, 4, (3, 3), stride=2)\n",
        "        self.conv1 = nn.Conv2d(4, 8, (3,3), stride=2)\n",
        "        self.conv2 = nn.Conv2d(8, 16, (3,3), stride=2)\n",
        "        self.flat = nn.Flatten(start_dim=1, end_dim=-1)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, input):\n",
        "        c0 = F.relu(self.conv0(input))\n",
        "        c1 = F.relu(self.conv1(c0))\n",
        "        c2 = F.relu(self.conv2(c1))\n",
        "        c2_flat = self.flat(c2)\n",
        "        feat_vec = self.linear(c2_flat)\n",
        "        logits = F.softmax(feat_vec, dim=1)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "3ZC5brvjZcXK"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = CNN_classifier(input_dim=(1, 28, 28), num_classes=10).to(DEVICE)"
      ],
      "metadata": {
        "id": "HF3USuX1a5sY"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(classifier_model, (1, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSGRlfdyCKVW",
        "outputId": "9d739d1d-5bd9-4a79-8777-beb39eaaf610"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN_classifier                           [1, 10]                   --\n",
              "├─Conv2d: 1-1                            [1, 4, 13, 13]            40\n",
              "├─Conv2d: 1-2                            [1, 8, 6, 6]              296\n",
              "├─Conv2d: 1-3                            [1, 16, 2, 2]             1,168\n",
              "├─Flatten: 1-4                           [1, 64]                   --\n",
              "├─Linear: 1-5                            [1, 10]                   650\n",
              "==========================================================================================\n",
              "Total params: 2,154\n",
              "Trainable params: 2,154\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.02\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.01\n",
              "Estimated Total Size (MB): 0.02\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prepare for Training"
      ],
      "metadata": {
        "id": "P3HlMMNnCO2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "num_epochs = 1\n",
        "MSE_loss = nn.MSELoss()"
      ],
      "metadata": {
        "id": "1rQClPTdCMED"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import LBFGS, Adam\n",
        "\n",
        "BCE_loss = nn.BCELoss()\n",
        "MSE_loss = nn.MSELoss()\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var, recon_const=1.):\n",
        "    #reconstruction_loss = recon_const*nn.functional.binary_cross_entropy(x_hat, x, reduction='sum',)\n",
        "    reconstruction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    return reconstruction_loss + KLD, reconstruction_loss, KLD\n",
        "\n",
        "\n",
        "optimizer = Adam(classifier_model.parameters(), lr=learning_rate)\n",
        "#optimizer = optim.LBFGS([rand_img])\n",
        "loss = nn.MSELoss()"
      ],
      "metadata": {
        "id": "HNRmBBCkF2LX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training"
      ],
      "metadata": {
        "id": "GctYx1veEqbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for batch_idx, (x, Y) in enumerate(tqdm(train_loader, desc=\"Training Epoch \"+str(epoch))):\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        one_hot = F.one_hot(Y, num_classes=10)\n",
        "\n",
        "        label = torch.tensor(one_hot, dtype=torch.float).to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = classifier_model(x)\n",
        "\n",
        "        loss = MSE_loss(label, logits)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.6f}'.format(epoch+1, num_epochs, loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBpg9eqzEn3G",
        "outputId": "b58593dd-faa1-4cb9-b730-35c62fe12f69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 0:   0%|          | 0/7500 [00:00<?, ?it/s]<ipython-input-13-ff3af4e9ca6f>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(one_hot, dtype=torch.float).to(DEVICE)\n",
            "<ipython-input-8-8cc3e95866bc>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = F.softmax(feat_vec)\n",
            "Training Epoch 0: 100%|██████████| 7500/7500 [00:42<00:00, 178.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.089502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Testing"
      ],
      "metadata": {
        "id": "owdP5ikeniyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_true = []\n",
        "labels_pred = []\n",
        "\n",
        "for batch_idx, (x, Y) in enumerate(tqdm(test_loader, desc=\"Testing \")):\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        one_hot = F.one_hot(Y, num_classes=10)\n",
        "\n",
        "        #label = torch.tensor(Y, dtype=torch.float).to(DEVICE)\n",
        "        labels_true.extend(Y.detach().numpy())\n",
        "\n",
        "        logits = classifier_model(x)\n",
        "        labels_pred.extend(torch.argmax(logits, axis=1).detach().numpy())\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtcVT-DGje7P",
        "outputId": "ca2a333a-873c-4abe-c595-d3e13eca4b47"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing :   0%|          | 0/1250 [00:00<?, ?it/s]<ipython-input-5-8cc3e95866bc>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = F.softmax(feat_vec)\n",
            "Testing :   0%|          | 0/1250 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiclass_f1_score(torch.tensor(labels_true), torch.tensor(labels_pred), num_classes=10, average=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsya3Cx2pOGp",
        "outputId": "ffecc969-f56a-4d41-d391-5c7fd8a6e570"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.2063, 0.0686, 0.0069, 0.0000, 0.0000, 0.0000, 0.0000, 0.0152,\n",
              "        0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Loss Landscape"
      ],
      "metadata": {
        "id": "S9RNHI-tpvaV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUULBFXcpcLM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Hessian"
      ],
      "metadata": {
        "id": "3tJ3-KRCrkgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.func import functional_call, vmap, hessian"
      ],
      "metadata": {
        "id": "KzVrj1qlrlTl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.autograd.functional.hessian(error_func, set of values)\n"
      ],
      "metadata": {
        "id": "ULDTNdHDupSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_params = dict(classifier_model.named_parameters())"
      ],
      "metadata": {
        "id": "h8MgRCvpvQQF"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = classifier_model\n",
        "\n",
        "batch_size=8\n",
        "\n",
        "targets = torch.randn(batch_size)\n",
        "inputs = torch.randn(batch_size, 1)\n",
        "params = dict(net.named_parameters())\n",
        "\n",
        "def fcall(params, inputs):\n",
        "  outputs = functional_call(net, params, inputs)\n",
        "  return outputs\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "  return torch.mean((outputs - targets)**2, dim=0)\n",
        "\n",
        "def compute_loss(params, inputs, targets):\n",
        "  #outputs = vmap(fcall, in_dims=(None,0))(params, inputs) #vectorize over batch\n",
        "  outputs = net(inputs)\n",
        "  return loss_fn(outputs, targets)\n",
        "\n",
        "def compute_hessian_loss(params, inputs, targets):\n",
        "  return hessian(compute_loss, argnums=(0))(params, inputs, targets)\n",
        "\n",
        "loss = compute_loss(my_params, x, one_hot)\n",
        "print(loss)\n",
        "\n",
        "hess = compute_hessian_loss(my_params, x, one_hot)\n",
        "key=list(params.keys())[0] #take weight in first layer as example key\n",
        "print(hess[key][key].shape) #Hessian of loss w.r.t first weight (shape [16, 1, 16, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDGh1_vj0HPF",
        "outputId": "62242c81-48cc-43a0-f035-4a782052a682"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1101, 0.2043, 0.1092, 0.0088, 0.2086, 0.0090, 0.0097, 0.1095, 0.0074,\n",
            "        0.1112], grad_fn=<MeanBackward1>)\n",
            "torch.Size([10, 4, 1, 3, 3, 4, 1, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY5N8gaGBsaq",
        "outputId": "cb199bd0-16b8-4ddb-f480-080c1aa5e23e"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.func import functional_call, vmap, hessian\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.fc1=nn.Linear(1,16)\n",
        "    self.fc2=nn.Linear(16,1)\n",
        "    self.af=nn.Tanh()\n",
        "  def forward(self, x):\n",
        "    x=self.fc1(x)\n",
        "    x=self.af(x)\n",
        "    x=self.fc2(x)\n",
        "    return x.squeeze(-1)\n",
        "\n",
        "net = Model()\n",
        "\n",
        "batch_size=1\n",
        "\n",
        "targets = torch.randn(batch_size)\n",
        "inputs = torch.randn(batch_size, 1)\n",
        "params = dict(net.named_parameters())\n",
        "\n",
        "def fcall(params, inputs):\n",
        "  outputs = functional_call(net, params, inputs)\n",
        "  return outputs\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "  return torch.mean((outputs - targets)**2, dim=0)\n",
        "\n",
        "def compute_loss(params, inputs, targets):\n",
        "  outputs = vmap(fcall, in_dims=(None,0))(params, inputs) #vectorize over batch\n",
        "  return loss_fn(outputs, targets)\n",
        "\n",
        "def compute_hessian_loss(params, inputs, targets):\n",
        "  return hessian(compute_loss, argnums=(0))(params, inputs, targets)\n",
        "\n",
        "loss = compute_loss(params, inputs, targets)\n",
        "print(loss)\n",
        "\n",
        "hess = compute_hessian_loss(params, inputs, targets)\n",
        "key=list(params.keys())[0] #take weight in first layer as example key\n",
        "print(hess[key][key].shape) #Hessian of loss w.r.t first weight (shape [16, 1, 16, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2pcWtnLFtj4",
        "outputId": "12e2a04d-6d43-45ea-a095-a404e5fcf634"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9320, grad_fn=<MeanBackward1>)\n",
            "torch.Size([16, 1, 16, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WpA4SDBHGVck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}